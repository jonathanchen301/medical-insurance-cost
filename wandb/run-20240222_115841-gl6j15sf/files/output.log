/Users/jonathan/projects/medical_insurance_cost/src/dataset.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  return torch.tensor(x_copy, dtype=torch.float32), torch.tensor(
/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Training Epoch 1...
Train Loss: 178013964.1047657 | Dev Loss: 136412179.8880597
Training Epoch 2...
Train Loss: 169565980.45696798 | Dev Loss: 130028451.89972015
Training Epoch 3...
Train Loss: 162089432.6412798 | Dev Loss: 124205338.46641791
Training Epoch 4...
Train Loss: 155200310.2774766 | Dev Loss: 118839413.61240672
Training Epoch 5...
Train Loss: 148809319.83718508 | Dev Loss: 113866498.2110541
Training Epoch 6...
Train Loss: 142859171.44279426 | Dev Loss: 109243023.51241837
Training Epoch 7...
Train Loss: 137309126.63811812 | Dev Loss: 104936600.203125
Training Epoch 8...
Train Loss: 132127289.38321185 | Dev Loss: 100921185.74020886
Training Epoch 9...
Train Loss: 127286841.376395 | Dev Loss: 97174628.45543887
Training Epoch 10...
Train Loss: 122764027.8762739 | Dev Loss: 93677407.44392782
Training Epoch 11...
Train Loss: 118537288.58331402 | Dev Loss: 90411902.00446744
Training Epoch 12...
Train Loss: 114586739.56901336 | Dev Loss: 87362018.16170308
Training Epoch 13...
Train Loss: 110893919.57408819 | Dev Loss: 84512920.15031774
Training Epoch 14...
Train Loss: 107441598.52442102 | Dev Loss: 81850925.26512943
Training Epoch 15...
Train Loss: 104213741.36078252 | Dev Loss: 79363306.89551328
Training Epoch 16...
Train Loss: 101195309.98622209 | Dev Loss: 77038265.72135174
Training Epoch 17...
Train Loss: 98372374.96200836 | Dev Loss: 74864861.59864739
Training Epoch 18...
Train Loss: 95731878.28698531 | Dev Loss: 72832897.72921206
Training Epoch 19...
Train Loss: 93261622.80012493 | Dev Loss: 70932905.24229662
Training Epoch 20...
Train Loss: 90950308.24885175 | Dev Loss: 69156012.81716418
Training Epoch 21...
Train Loss: 88787321.00214472 | Dev Loss: 67493981.99952629
Training Epoch 22...
Train Loss: 86762790.84336728 | Dev Loss: 65939174.32486007
Training Epoch 23...
Train Loss: 84867557.89966796 | Dev Loss: 64484472.79716651
Training Epoch 24...
Train Loss: 83093030.28952485 | Dev Loss: 63123168.723530784
Training Epoch 25...
Train Loss: 81431227.38576628 | Dev Loss: 61849087.283421755
Training Epoch 26...
Train Loss: 79874679.88832967 | Dev Loss: 60656427.896506235
Training Epoch 27...
Train Loss: 78416418.52753013 | Dev Loss: 59539776.05234812
Training Epoch 28...
Train Loss: 77050001.13388161 | Dev Loss: 58494106.18613582
Training Epoch 29...
Train Loss: 75769340.52001289 | Dev Loss: 57514728.10228217
Training Epoch 30...
Train Loss: 74568841.91038091 | Dev Loss: 56597271.34107538
Training Epoch 31...
Train Loss: 73443228.96156593 | Dev Loss: 55737622.63542152
Training Epoch 32...
Train Loss: 72387572.70163679 | Dev Loss: 54931966.84791861
Training Epoch 33...
Train Loss: 71397322.77176152 | Dev Loss: 54176785.48673624
Training Epoch 34...
Train Loss: 70468231.01696347 | Dev Loss: 53468722.24769262
Training Epoch 35...
Train Loss: 69596273.64567064 | Dev Loss: 52804697.499868594
Training Epoch 36...
Train Loss: 68777753.3211812 | Dev Loss: 52181799.14093518
Training Epoch 37...
Train Loss: 68009196.0605527 | Dev Loss: 51597357.36675114
Training Epoch 38...
Train Loss: 67287371.23625186 | Dev Loss: 51048814.486498736
Training Epoch 39...
Train Loss: 66609237.60299392 | Dev Loss: 50533811.33771939
Training Epoch 40...
Train Loss: 65971907.85260553 | Dev Loss: 50050130.677559465
Training Epoch 41...
Train Loss: 65372806.18625929 | Dev Loss: 49595786.190803185
Training Epoch 42...
Train Loss: 64809477.803457 | Dev Loss: 49168810.73038312
Training Epoch 43...
Train Loss: 64279619.98645567 | Dev Loss: 48767440.82274662
Training Epoch 44...
Train Loss: 63781080.43062088 | Dev Loss: 48390004.72969989
Training Epoch 45...
Train Loss: 63311835.6309993 | Dev Loss: 48034916.72048631
Training Epoch 46...
Train Loss: 62870000.351974666 | Dev Loss: 47700726.80765454
Training Epoch 47...
Train Loss: 62453867.10890021 | Dev Loss: 47386111.604889825
Training Epoch 48...
Train Loss: 62061767.9153788 | Dev Loss: 47089716.02245012
Training Epoch 49...
Train Loss: 61692157.92582745 | Dev Loss: 46810440.01422302
Training Epoch 50...
Train Loss: 61343658.38492577 | Dev Loss: 46547131.80723549
Training Epoch 51...
Train Loss: 61014920.51889137 | Dev Loss: 46298790.47053551
Training Epoch 52...
Train Loss: 60704699.72478362 | Dev Loss: 46064436.29383964
Training Epoch 53...
Train Loss: 60411824.320481 | Dev Loss: 45843139.01666715
Training Epoch 54...
Train Loss: 60135181.58004179 | Dev Loss: 45634054.63973042
Training Epoch 55...
Train Loss: 59873768.83399124 | Dev Loss: 45436428.06987706
Training Epoch 56...
Train Loss: 59626647.36359314 | Dev Loss: 45249486.9910916
Training Epoch 57...
Train Loss: 59392870.151837066 | Dev Loss: 45072533.51514219
Training Epoch 58...
Train Loss: 59171651.79435274 | Dev Loss: 44904970.55141529
Training Epoch 59...
Train Loss: 58962218.32490332 | Dev Loss: 44746172.699418254
Training Epoch 60...
Train Loss: 58763823.2117388 | Dev Loss: 44595570.48609868
Training Epoch 61...
Train Loss: 58575781.51652756 | Dev Loss: 44452662.373981476
Training Epoch 62...
Train Loss: 58397469.62313926 | Dev Loss: 44316916.04251663
Training Epoch 63...
Train Loss: 58228269.06943039 | Dev Loss: 44187921.58145506
Training Epoch 64...
Train Loss: 58067655.93022884 | Dev Loss: 44065231.59134321
Training Epoch 65...
Train Loss: 57915083.7668947 | Dev Loss: 43948442.07352641
Training Epoch 66...
Train Loss: 57770067.7023901 | Dev Loss: 43837183.12912306
Training Epoch 67...
Train Loss: 57632163.7691071 | Dev Loss: 43731114.74568017
Training Epoch 68...
Train Loss: 57500927.98993773 | Dev Loss: 43629889.203786366
Training Epoch 69...
Train Loss: 57375960.65196371 | Dev Loss: 43533208.07127813
Training Epoch 70...
Train Loss: 57256896.84428145 | Dev Loss: 43440820.186386794
Training Epoch 71...
Train Loss: 57143387.655672066 | Dev Loss: 43352425.725348175
Training Epoch 72...
Train Loss: 57035102.07535767 | Dev Loss: 43267771.05037314
Training Epoch 73...
Train Loss: 56931717.78617542 | Dev Loss: 43186639.22329029
Training Epoch 74...
Train Loss: 56832955.31394077 | Dev Loss: 43108800.40916329
Training Epoch 75...
Train Loss: 56738541.83206932 | Dev Loss: 43034068.09744308
Training Epoch 76...
Train Loss: 56648234.523238875 | Dev Loss: 42962234.60988879
Training Epoch 77...
Train Loss: 56561785.91246176 | Dev Loss: 42893141.679918885
Training Epoch 78...
Train Loss: 56478982.988776796 | Dev Loss: 42826613.22286759
Training Epoch 79...
Train Loss: 56399614.224680446 | Dev Loss: 42762498.89663036
Training Epoch 80...
Train Loss: 56323486.70338115 | Dev Loss: 42700643.46276776
Training Epoch 81...
Train Loss: 56250409.14684768 | Dev Loss: 42640925.92438199
Training Epoch 82...
Train Loss: 56180221.145661466 | Dev Loss: 42583211.489914395
Training Epoch 83...
Train Loss: 56112758.94389697 | Dev Loss: 42527380.16267873
Training Epoch 84...
Train Loss: 56047864.91855417 | Dev Loss: 42473327.441025466
Training Epoch 85...
Train Loss: 55985415.37774364 | Dev Loss: 42420956.721676044
Training Epoch 86...
Train Loss: 55925266.5890035 | Dev Loss: 42370143.65453737
Training Epoch 87...
Train Loss: 55867285.48961465 | Dev Loss: 42320834.59397228
Training Epoch 88...
Train Loss: 55811367.146573074 | Dev Loss: 42272925.09240905
Training Epoch 89...
Train Loss: 55757399.142429546 | Dev Loss: 42226332.220703125
Training Epoch 90...
Train Loss: 55705286.15049283 | Dev Loss: 42180991.202957384
Training Epoch 91...
Train Loss: 55654924.2189446 | Dev Loss: 42136831.89158152
Training Epoch 92...
Train Loss: 55606215.86130971 | Dev Loss: 42093788.9588787
Training Epoch 93...
Train Loss: 55559093.999092564 | Dev Loss: 42051799.99779043
Training Epoch 94...
Train Loss: 55513469.89258541 | Dev Loss: 42010820.31801776
Training Epoch 95...
Train Loss: 55469265.45140463 | Dev Loss: 41970766.50362546
Training Epoch 96...
Train Loss: 55426406.91183416 | Dev Loss: 41931632.518392876
Training Epoch 97...
Train Loss: 55384837.71162336 | Dev Loss: 41893342.72115246
Training Epoch 98...
Train Loss: 55344485.68511179 | Dev Loss: 41855852.08604955
Training Epoch 99...
Train Loss: 55305296.35341814 | Dev Loss: 41819134.84168209
Training Epoch 100...
Train Loss: 55267213.47908944 | Dev Loss: 41783146.56621859
Training Epoch 101...
Train Loss: 55230190.28854985 | Dev Loss: 41747848.83333941
Training Epoch 102...
Train Loss: 55194181.985140584 | Dev Loss: 41713219.68510414
Training Epoch 103...
Train Loss: 55159125.55372546 | Dev Loss: 41679213.610344276
Training Epoch 104...
Train Loss: 55124990.78724683 | Dev Loss: 41645813.25139197
Training Epoch 105...
Train Loss: 55091735.51966933 | Dev Loss: 41612990.78641157
Training Epoch 106...
Train Loss: 55059319.11185231 | Dev Loss: 41580714.69708161
Training Epoch 107...
Train Loss: 55027707.987654015 | Dev Loss: 41548964.79165513
Training Epoch 108...
Train Loss: 54996855.00435072 | Dev Loss: 41517701.17327243
Training Epoch 109...
Train Loss: 54966736.86875327 | Dev Loss: 41486925.3903098
Training Epoch 110...
Train Loss: 54937329.22116416 | Dev Loss: 41456601.28280594
Training Epoch 111...
Train Loss: 54908593.9180064 | Dev Loss: 41426733.05571143
Training Epoch 112...
Train Loss: 54880513.855309404 | Dev Loss: 41397288.95506082
Training Epoch 113...
Train Loss: 54853059.52404868 | Dev Loss: 41368246.830679365
Training Epoch 114...
Train Loss: 54826207.753321424 | Dev Loss: 41339617.73591272
Training Epoch 115...
Train Loss: 54799927.806613006 | Dev Loss: 41311356.00284132
Training Epoch 116...
Train Loss: 54774204.222149685 | Dev Loss: 41283465.39255353
Training Epoch 117...
Train Loss: 54749018.59232255 | Dev Loss: 41255932.998695485
Training Epoch 118...
Train Loss: 54724345.80381082 | Dev Loss: 41228728.40174251
Training Epoch 119...
Train Loss: 54700167.145021126 | Dev Loss: 41201854.20665957
Training Epoch 120...
Train Loss: 54676471.22305338 | Dev Loss: 41175310.37725557
Training Epoch 121...
Train Loss: 54653239.27160261 | Dev Loss: 41149073.12328054
Training Epoch 122...
Train Loss: 54630453.096061155 | Dev Loss: 41123144.85116213
Training Epoch 123...
Train Loss: 54608098.807484105 | Dev Loss: 41097498.234561294
Training Epoch 124...
Train Loss: 54586160.676970266 | Dev Loss: 41072136.31639896
Training Epoch 125...
Train Loss: 54564629.45303105 | Dev Loss: 41047058.41427795
Training Epoch 126...
Train Loss: 54543490.46721215 | Dev Loss: 41022231.862558395
Training Epoch 127...
Train Loss: 54522734.4097891 | Dev Loss: 40997688.51850413
Training Epoch 128...
Train Loss: 54502344.32317626 | Dev Loss: 40973393.90356992
Training Epoch 129...
Train Loss: 54482318.67815288 | Dev Loss: 40949353.16362751
Training Epoch 130...
Train Loss: 54462635.68812785 | Dev Loss: 40925549.84264146
Training Epoch 131...
Train Loss: 54443287.49658718 | Dev Loss: 40901989.728282414
Training Epoch 132...
Train Loss: 54424270.568302736 | Dev Loss: 40878665.66261792
Training Epoch 133...
Train Loss: 54405571.10291874 | Dev Loss: 40855567.17360903
Training Epoch 134...
Train Loss: 54387187.50924275 | Dev Loss: 40832691.48189454
Training Epoch 135...
Train Loss: 54369098.63096044 | Dev Loss: 40810032.14214029
Training Epoch 136...
Train Loss: 54351307.25781561 | Dev Loss: 40787590.90427262
Training Epoch 137...
Train Loss: 54333806.67544011 | Dev Loss: 40765356.76877316
Training Epoch 138...
Train Loss: 54316577.604659006 | Dev Loss: 40743328.98582356
Training Epoch 139...
Train Loss: 54299624.83305309 | Dev Loss: 40721509.66890204
Training Epoch 140...
Train Loss: 54282930.168859355 | Dev Loss: 40699881.60448941
Training Epoch 141...
Train Loss: 54266496.83386491 | Dev Loss: 40678458.11451482
Training Epoch 142...
Train Loss: 54250320.72338799 | Dev Loss: 40657216.91226617
Training Epoch 143...
Train Loss: 54234391.34828855 | Dev Loss: 40636174.21666137
Training Epoch 144...
Train Loss: 54218703.08339966 | Dev Loss: 40615325.499514565
Training Epoch 145...
Train Loss: 54203254.782958925 | Dev Loss: 40594653.58338336
Training Epoch 146...
Train Loss: 54188032.816330485 | Dev Loss: 40574156.73072063
Training Epoch 147...
Train Loss: 54173036.26693932 | Dev Loss: 40553850.93032108
Training Epoch 148...
Train Loss: 54158260.90379504 | Dev Loss: 40533716.059565075
Training Epoch 149...
Train Loss: 54143702.467223525 | Dev Loss: 40513748.765430056
Training Epoch 150...
Train Loss: 54129355.25576255 | Dev Loss: 40493963.51679514
Training Epoch 151...
Train Loss: 54115214.485543765 | Dev Loss: 40474340.901888944
Training Epoch 152...
Train Loss: 54101275.89653517 | Dev Loss: 40454886.727553636
Training Epoch 153...
Train Loss: 54087540.7943947 | Dev Loss: 40435598.21672491
Training Epoch 154...
Train Loss: 54073999.30327707 | Dev Loss: 40416468.24012506
Training Epoch 155...
Train Loss: 54060644.6074144 | Dev Loss: 40397505.91166573
Training Epoch 156...
Train Loss: 54047483.48813785 | Dev Loss: 40378698.50470927
Training Epoch 157...
Train Loss: 54034509.592291184 | Dev Loss: 40360050.06165792
Training Epoch 158...
Traceback (most recent call last):
  File "/Users/jonathan/projects/medical_insurance_cost/src/train.py", line 33, in <module>
    model.learn(train_dataloader, dev_dataloader, num_epochs, optimizer, loss_fct)
  File "/Users/jonathan/projects/medical_insurance_cost/src/model.py", line 31, in learn
    optimizer.step()
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/optim/sgd.py", line 75, in step
    sgd(params_with_grad,
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/optim/sgd.py", line 220, in sgd
    func(params,
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/optim/sgd.py", line 263, in _single_tensor_sgd
    param.add_(d_p, alpha=-lr)
KeyboardInterrupt