/Users/jonathan/projects/medical_insurance_cost/src/dataset.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  return torch.tensor(x_copy, dtype=torch.float32), torch.tensor(
/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Training Epoch 1...
Train Loss: 169252482.98064223 | Dev Loss: 140975528.4085821
Training Epoch 2...
Train Loss: 127787496.41884857 | Dev Loss: 138847975.1380597
Training Epoch 3...
Train Loss: 125969715.9674111 | Dev Loss: 136756684.32462686
Training Epoch 4...
Train Loss: 124098249.02076837 | Dev Loss: 134560521.58582088
Training Epoch 5...
Train Loss: 122101934.92011034 | Dev Loss: 132196366.41231343
Training Epoch 6...
Train Loss: 119924495.45185547 | Dev Loss: 129600789.84235075
Training Epoch 7...
Train Loss: 117505668.729745 | Dev Loss: 126702004.93003732
Training Epoch 8...
Train Loss: 114776409.13152826 | Dev Loss: 123416920.64272387
Training Epoch 9...
Train Loss: 111657643.11128066 | Dev Loss: 119651030.66884328
Training Epoch 10...
Train Loss: 108062167.46985571 | Dev Loss: 115302521.96548508
Training Epoch 11...
Train Loss: 103901352.7079987 | Dev Loss: 110272905.7602612
Training Epoch 12...
Train Loss: 99098692.37481792 | Dev Loss: 104486771.83290578
Training Epoch 13...
Train Loss: 93611656.88262266 | Dev Loss: 97921648.12622435
Training Epoch 14...
Train Loss: 87461052.1866973 | Dev Loss: 90645439.3551991
Training Epoch 15...
Train Loss: 80762585.58986554 | Dev Loss: 82851859.4764459
Training Epoch 16...
Train Loss: 73749410.19433662 | Dev Loss: 74877847.28561538
Training Epoch 17...
Train Loss: 66769791.92942787 | Dev Loss: 67181360.28291161
Training Epoch 18...
Train Loss: 60244403.89678774 | Dev Loss: 60263615.95522388
Training Epoch 19...
Train Loss: 54579957.77365212 | Dev Loss: 54543145.07513701
Training Epoch 20...
Train Loss: 50060493.62960694 | Dev Loss: 50230147.54154945
Training Epoch 21...
Train Loss: 46763285.20754921 | Dev Loss: 47270893.78807332
Training Epoch 22...
Train Loss: 44563375.31721859 | Dev Loss: 45412669.22839538
Training Epoch 23...
Train Loss: 43193890.270530954 | Dev Loss: 44305162.43353009
Training Epoch 24...
Train Loss: 42372068.03307773 | Dev Loss: 43702311.87265151
Training Epoch 25...
Train Loss: 41867853.23966674 | Dev Loss: 43383798.62439512
Training Epoch 26...
Train Loss: 41506962.673486285 | Dev Loss: 43185433.50301714
Training Epoch 27...
Train Loss: 41257149.59150686 | Dev Loss: 42948282.413484745
Training Epoch 28...
Train Loss: 41058491.478283636 | Dev Loss: 42720704.559759796
Training Epoch 29...
Train Loss: 40895296.165948875 | Dev Loss: 42563815.55519582
Training Epoch 30...
Train Loss: 40753332.89635054 | Dev Loss: 42380777.72145376
Training Epoch 31...
Train Loss: 40619888.17952499 | Dev Loss: 42237698.90758822
Training Epoch 32...
Train Loss: 40497284.676559865 | Dev Loss: 42068146.049914
Training Epoch 33...
Train Loss: 40375626.66677329 | Dev Loss: 41927258.14747927
Training Epoch 34...
Train Loss: 40258929.85755804 | Dev Loss: 41783870.720830604
Training Epoch 35...
Train Loss: 40142747.64611696 | Dev Loss: 41647234.21585971
Training Epoch 36...
Train Loss: 40026011.001943074 | Dev Loss: 41509687.755832046
Training Epoch 37...
Train Loss: 39911032.1459685 | Dev Loss: 41380502.30921184
Training Epoch 38...
Train Loss: 39806778.44374325 | Dev Loss: 41246554.0549691
Training Epoch 39...
Train Loss: 39696462.41393785 | Dev Loss: 41123175.46698909
Training Epoch 40...
Train Loss: 39583791.181082554 | Dev Loss: 41037568.91449794
Training Epoch 41...
Train Loss: 39481215.09657149 | Dev Loss: 40915752.55114746
Training Epoch 42...
Train Loss: 39383841.27450485 | Dev Loss: 40796845.033266895
Training Epoch 43...
Train Loss: 39298095.44489645 | Dev Loss: 40675003.29549761
Training Epoch 44...
Train Loss: 39199687.69432339 | Dev Loss: 40560921.70560558
Training Epoch 45...
Train Loss: 39103284.50650415 | Dev Loss: 40464907.83706756
Training Epoch 46...
Train Loss: 39011669.10381523 | Dev Loss: 40360731.876135185
Training Epoch 47...
Train Loss: 38924166.13507033 | Dev Loss: 40261546.29102793
Training Epoch 48...
Train Loss: 38836545.17652478 | Dev Loss: 40150565.12226299
Training Epoch 49...
Train Loss: 38744089.93357809 | Dev Loss: 40062477.53555198
Training Epoch 50...
Train Loss: 38658744.166408315 | Dev Loss: 39969190.01583823
Training Epoch 51...
Train Loss: 38586501.457846284 | Dev Loss: 39871666.71574783
Training Epoch 52...
Train Loss: 38507102.83073967 | Dev Loss: 39780348.33296272
Training Epoch 53...
Train Loss: 38431389.92241717 | Dev Loss: 39684243.71955969
Training Epoch 54...
Train Loss: 38360756.298177004 | Dev Loss: 39601840.80417827
Training Epoch 55...
Train Loss: 38285102.88220608 | Dev Loss: 39516453.89447659
Training Epoch 56...
Train Loss: 38207958.507848665 | Dev Loss: 39445445.24267031
Training Epoch 57...
Train Loss: 38133528.836984515 | Dev Loss: 39376964.45062711
Training Epoch 58...
Train Loss: 38069359.67797963 | Dev Loss: 39301356.543730326
Training Epoch 59...
Train Loss: 37999905.133331515 | Dev Loss: 39224842.70298653
Training Epoch 60...
Train Loss: 37933685.85256671 | Dev Loss: 39155902.78914267
Training Epoch 61...
Train Loss: 37867338.28961724 | Dev Loss: 39084398.68810124
Training Epoch 62...
Train Loss: 37801598.58235389 | Dev Loss: 39014870.93593313
Training Epoch 63...
Train Loss: 37737333.53489025 | Dev Loss: 38946358.66065182
Training Epoch 64...
Train Loss: 37670728.168265454 | Dev Loss: 38881166.44153561
Training Epoch 65...
Train Loss: 37602507.49820893 | Dev Loss: 38812346.24895967
Training Epoch 66...
Train Loss: 37537044.60138141 | Dev Loss: 38751838.20376627
Training Epoch 67...
Train Loss: 37472894.05314724 | Dev Loss: 38685865.98777566
Training Epoch 68...
Train Loss: 37412241.28827035 | Dev Loss: 38623410.69943488
Training Epoch 69...
Train Loss: 37352016.27296371 | Dev Loss: 38557911.64873184
Training Epoch 70...
Train Loss: 37284296.838488914 | Dev Loss: 38499650.21558955
Training Epoch 71...
Train Loss: 37220841.74638128 | Dev Loss: 38436011.51037307
Training Epoch 72...
Traceback (most recent call last):
  File "/Users/jonathan/projects/medical_insurance_cost/src/train.py", line 38, in <module>
    model.learn(train_dataloader, dev_dataloader, num_epochs, optimizer, loss_fct)
  File "/Users/jonathan/projects/medical_insurance_cost/src/model.py", line 99, in learn
    predictions = self.forward(batch_X)
  File "/Users/jonathan/projects/medical_insurance_cost/src/model.py", line 85, in forward
    x = self.linear2(x)
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/jonathan/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 115, in forward
    def forward(self, input: Tensor) -> Tensor:
KeyboardInterrupt
Train Loss: 37159405.872007534 | Dev Loss: 38371435.53636659
Training Epoch 73...
Train Loss: 37101006.60751073 | Dev Loss: 38310965.94728179
Training Epoch 74...